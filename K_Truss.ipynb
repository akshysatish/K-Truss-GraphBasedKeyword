{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kDbQbTFRF4X-",
        "outputId": "92935094-b488-44f0-aa37-8dff34440b14"
      },
      "outputs": [],
      "source": [
        "!pip install textacy\n",
        "!pip install rake-nltk\n",
        " \n",
        "import networkx as nx\n",
        "import re \n",
        "import itertools\n",
        "import operator\n",
        "import copy\n",
        "import heapq\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag\n",
        "import string\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from networkx import k_core\n",
        "from networkx import core_number\n",
        "import spacy\n",
        "import sys\n",
        "import numpy as np\n",
        "import textacy\n",
        "from nltk.tokenize import word_tokenize,wordpunct_tokenize\n",
        "import networkx.drawing\n",
        "#import textacy.keyterms\n",
        "from textacy import preprocessing\n",
        "from networkx import common_neighbors\n",
        "import pandas as pd\n",
        "from collections import OrderedDict\n",
        "nltk.download('stopwords')\n",
        " \n",
        "nlp = spacy.load('en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "5D99L5BcD1df",
        "outputId": "cc219c5c-6c8e-41d6-b9d2-187b4ac708ec"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "A5YEQoJoG3LU",
        "outputId": "14a7bb6b-1f5a-474f-c032-8675ff38c464"
      },
      "outputs": [],
      "source": [
        "!pip install emoji\n",
        "nb_keys_terms_needed=50\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#tweetdf = pd.read_csv('/content/drive/My Drive/opntwweets.csv')\n",
        "tweetdf = pd.read_csv('/content/opntwweets.csv')\n",
        "print(len(tweetdf))\n",
        "\n",
        "txt_sentences = tweetdf[0:9].tweet.values\n",
        "print(txt_sentences)\n",
        "\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "spacy_nlp = spacy.load('en_core_web_sm')\n",
        "nlp = English()\n",
        "# Create a blank Tokenizer with just the English vocab\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "punct = set(string.punctuation)\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "ignore_chars = set(['\\n',' ','...','\"',''])\n",
        "dig_pattern = re.compile('[0-9]')\n",
        "cleaned_text=[]\n",
        "for i,phrase in enumerate(txt_sentences):\n",
        "  tokens=[]\n",
        "  phrase = re.sub(r\"http\\S+\", \"\", phrase)\n",
        "  phrase = dig_pattern.sub(r'',phrase)\n",
        "  ed_phrase = emoji_pattern.sub(r'', phrase)\n",
        "  doc = spacy_nlp(ed_phrase)\n",
        "  tokens = [token.text for token in doc]\n",
        "  tokens = [token for token in tokens if token not in stop_words]\n",
        "  tokens = [token for token in tokens if token not in punct]\n",
        "  tokens = [token for token in tokens if token not in ignore_chars]\n",
        "  cleaned_text.append(tokens)\n",
        "\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bLvJdpFffSj"
      },
      "outputs": [],
      "source": [
        "phrasesdf = pd.read_csv('/content/drive/My Drive/texts - Sheet1.csv')\n",
        "phrasesdf.columns = ['text']\n",
        "\n",
        "tweets = phrasesdf.text.values\n",
        "text = ''\n",
        "for tweet in tweets:\n",
        "  text+=tweet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "D2DDouPyHoYw",
        "outputId": "d72af034-cdcd-4a56-911b-a6e5ac498e27"
      },
      "outputs": [],
      "source": [
        "def terms_to_graph_sents(clean_txt_sents, w,stopping_end_of_line=0):\n",
        "    from_to = {}\n",
        "    if(not stopping_end_of_line):\n",
        "        extended_clean_txt_sents=[]\n",
        "        for sublist in clean_txt_sents:\n",
        "            extended_clean_txt_sents.extend(sublist)\n",
        "        clean_txt_sents=[extended_clean_txt_sents]\n",
        "    for k,sents in enumerate(clean_txt_sents):\n",
        "        len_sents=len(sents)\n",
        "\n",
        "        # create initial complete graph (first w terms)\n",
        "        terms_temp = sents[0:min(w,len_sents)]\n",
        "        indexes = list(itertools.combinations(range(min(w,len_sents)), r=2))\n",
        "        new_edges = []\n",
        "        for my_tuple in indexes:\n",
        "            new_edges.append(tuple([terms_temp[i] for i in my_tuple]))\n",
        "\n",
        "        for new_edge in new_edges:\n",
        "            if new_edge in from_to:\n",
        "                from_to[new_edge] += 1\n",
        "            else:\n",
        "                from_to[new_edge] = 1\n",
        "\n",
        "        if(w<=len_sents):\n",
        "            # then iterate over the remaining terms\n",
        "            for i in range(w, len_sents):\n",
        "                considered_term = sents[i] # term to consider\n",
        "                terms_temp = sents[(i-w+1):(i+1)] # all terms within sliding window\n",
        "\n",
        "                # edges to try\n",
        "                candidate_edges = []\n",
        "                for p in range(w-1):\n",
        "                    candidate_edges.append((terms_temp[p],considered_term))\n",
        "\n",
        "                for try_edge in candidate_edges:\n",
        "\n",
        "                    if try_edge[1] != try_edge[0]:\n",
        "                    # if not self-edge\n",
        "\n",
        "                        # if edge has already been seen, update its weight\n",
        "                        if try_edge in from_to:\n",
        "                            from_to[try_edge] += 1\n",
        "\n",
        "                        # if edge has never been seen, create it and assign it a unit weight     \n",
        "                        else:\n",
        "                            from_to[try_edge] = 1\n",
        "    \n",
        "    return(from_to)\n",
        "\n",
        "tuples_words_sents_unweighted=list(terms_to_graph_sents(cleaned_text, w=5,stopping_end_of_line=1).keys())\n",
        "tuples_words_sents_weighted=terms_to_graph_sents(cleaned_text, w=5,stopping_end_of_line=0)\n",
        "\n",
        "print(\"tuples unweighted \\n\")\n",
        "tuples_words_sents_unweighted[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "LI80ZH6xUO5o",
        "outputId": "fdc90ec0-2903-4fe3-a502-9c97a6a50a92"
      },
      "outputs": [],
      "source": [
        "nb_iter=0\n",
        "print(\"tuples weighted \\n\")\n",
        "for key in tuples_words_sents_weighted.keys():\n",
        "    if(nb_iter <10):\n",
        "        nb_iter+=1\n",
        "        print(key,tuples_words_sents_weighted[key])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqYZ-jqtUTyq"
      },
      "outputs": [],
      "source": [
        "\n",
        "## UNWEIGHTED AND WEIGHTED GRAPH \n",
        "def unweighted_graph(tuples_words_sents_unweighted):\n",
        "    G = nx.Graph()\n",
        "    G.add_edges_from(tuples_words_sents_unweighted)\n",
        "    G.remove_edges_from(nx.selfloop_edges(G))\n",
        "    return(G)\n",
        "\n",
        "G_unweighted=unweighted_graph(tuples_words_sents_unweighted)\n",
        "\n",
        "def weighted_graph(tuples_words_sents_weighted):\n",
        "    G = nx.Graph()\n",
        "    for keys,values in tuples_words_sents_weighted.items():\n",
        "        G.add_edge(keys[0],keys[1],weight=values)\n",
        "        G.remove_edges_from(nx.selfloop_edges(G))\n",
        "    return(G)\n",
        "\n",
        "G_weighted=weighted_graph(tuples_words_sents_weighted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "FMYRAfrhUXqM",
        "outputId": "d14f24f7-65dd-4627-a686-5c4bfa5c0d41"
      },
      "outputs": [],
      "source": [
        "## Generate and order a list of best keywords, the number is specified by \"nb_keys_terms_needed\"\n",
        "G_Unweighted_core_number=core_number(G_unweighted)\n",
        "G_Weighted_core_number=core_number(G_weighted)\n",
        "\n",
        "def order_dict_best_keywords(G_core_number,nb_keys_terms_needed=10):\n",
        "    k_core_keyTerms=sorted(G_core_number, key=G_core_number.get, reverse=True)\n",
        "    Kcore_values=[G_core_number[x] for x in k_core_keyTerms[:nb_keys_terms_needed]]\n",
        "    return(k_core_keyTerms,Kcore_values)\n",
        "\n",
        "UW_k_core_keyTerms,UW_Kcore_values=order_dict_best_keywords(G_Unweighted_core_number,nb_keys_terms_needed)\n",
        "W_k_core_keyTerms,W_Kcore_values=order_dict_best_keywords(G_Weighted_core_number,nb_keys_terms_needed)\n",
        "print(UW_k_core_keyTerms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "Wf4mILqDVx8g",
        "outputId": "b559320f-59d2-4560-ec6e-efb538685fea"
      },
      "outputs": [],
      "source": [
        "## WEIGHTED \n",
        "nx.draw(G_weighted, with_labels=True,font_size=0.02,width=1,node_size=10,label='Graph Unweighted GOT')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "bVslsFd3V4jV",
        "outputId": "1923b6ec-0b4e-4d6f-bd1a-502725324b42"
      },
      "outputs": [],
      "source": [
        "\n",
        "## UNWEIGHTED\n",
        "nx.draw(G_unweighted, with_labels=True,font_size=0.02,width=1,node_size=10,label='Graph Unweighted GOT')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "4lo_jrlvV8yc",
        "outputId": "004380ef-45d4-49e2-aa8c-3d7e7b601ece"
      },
      "outputs": [],
      "source": [
        "subgraph_k_core=k_core(G_weighted, k=10 ,core_number=None)\n",
        "nx.draw(subgraph_k_core, with_labels=True,node_color='y',edge_color='g',font_size=10,width=1,node_size=500,label='Graph Unweighted GOT')\n",
        "plt.show()\n",
        "plt.savefig(\"subrgraph_kcore.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UtWKTI54W71h",
        "outputId": "ae24f815-1d14-41bc-b5fd-b25e505e8c70"
      },
      "outputs": [],
      "source": [
        "!pip install ir-evaluation-py\n",
        "import csv\n",
        "from networkx import closeness_centrality,degree_centrality\n",
        "from collections import Counter\n",
        "G=G_unweighted.copy()\n",
        "def compute_sup_e(H):\n",
        "    dict_res={}\n",
        "    edges_sorted={}\n",
        "    edges=H.edges\n",
        "    for e in edges:\n",
        "        edges_sorted[e]=len(list(common_neighbors(H, e[0], e[1])))\n",
        "    edges_sorted=sorted(edges_sorted.items(), key=lambda x: x[1])\n",
        "    return(OrderedDict(edges_sorted))\n",
        "\n",
        "def dict_to_sorted_dict(dictionnary):\n",
        "    return(OrderedDict(sorted(dictionnary.items(), key=lambda x: x[1])))\n",
        " \n",
        "def k_truss_True_Dec_Massive_Net(G,output_for_density=True):\n",
        "    edges_sorted=compute_sup_e(G)\n",
        "    print(edges_sorted)\n",
        "    keys, values = [], []\n",
        "\n",
        "    for key, value in edges_sorted.items():\n",
        "      keys.append(key)\n",
        "      values.append(value)       \n",
        "    print(len(values))\n",
        "    ranks, counts = np.unique(values, return_counts=True)\n",
        "    print(dict(zip(ranks,counts)))\n",
        "    with open(\"frequencies.csv\", \"w\") as outfile:\n",
        "      csvwriter = csv.writer(outfile)\n",
        "      csvwriter.writerow(keys)\n",
        "      csvwriter.writerow(values)\n",
        "    k_truss_nodes={}\n",
        "    for node in G.nodes:\n",
        "        k_truss_nodes[node]=0\n",
        "    not_all_edges_removed=True\n",
        "    edges_sorted=compute_sup_e(G)\n",
        "    k=2\n",
        "    liste_remove=[]\n",
        "    output_density=[[k,len(G.nodes()),len(G.edges)]]\n",
        "    while (not_all_edges_removed):\n",
        "        liste_remove_k=[]\n",
        "        while(list(edges_sorted.values())[0]<=k-2):\n",
        "            e=list(edges_sorted.keys())[0]\n",
        "            node1=list(edges_sorted.keys())[0][0]\n",
        "            node2=list(edges_sorted.keys())[0][1]\n",
        "            nb_u=G.neighbors(node1)\n",
        "            nb_v=G.neighbors(node2)\n",
        "            nbU=list(nb_v)\n",
        "            u=node2\n",
        "            v=node1\n",
        "            if (len(list(nb_u))<len(list(nb_v))) :\n",
        "                nbU=list(nb_u)\n",
        "                u=node1\n",
        "                v=node2\n",
        "            for w in nbU:\n",
        "                if(G.has_edge(w,v)):\n",
        "                    sup_keys=edges_sorted.keys()\n",
        "                    if((v,w) in sup_keys):\n",
        "                        edges_sorted[(v,w)]-=1\n",
        "                    else:\n",
        "                        edges_sorted[(w,v)]-=1\n",
        "                    if((u,w) in sup_keys):\n",
        "                        edges_sorted[(u,w)]-=1\n",
        "                    else:\n",
        "                        edges_sorted[(w,u)]-=1\n",
        "                    edges_sorted=dict_to_sorted_dict(edges_sorted)\n",
        "            del(edges_sorted[e])\n",
        "            liste_remove_k.append(e)\n",
        "            G.remove_edge(e[0],e[1])\n",
        "            if(len(G.edges)==0):\n",
        "                break\n",
        "        liste_remove.append(liste_remove_k)\n",
        "        \n",
        "        if(len(list(edges_sorted.keys()))==0):\n",
        "            not_all_edges_removed=False\n",
        "        else:\n",
        "            k+=1            \n",
        "            G_bis=max((G.subgraph(c) for c in nx.connected_components(G)),key=len)\n",
        "            for node_not_removed in G_bis.nodes:\n",
        "                k_truss_nodes[node_not_removed]+=1\n",
        "            if(output_for_density):\n",
        "                output_density.append([k,len(list(G_bis.nodes)),len(G_bis.edges)]) \n",
        "\n",
        "            nx.draw(G_bis, with_labels=True,font_color='k',node_color='g',edge_color='y',font_size=max(min(20,500/len(G_bis.nodes)),9),width=1,node_size=0,label='Graph Unweighted GOT')\n",
        "            #plt.figure(figsize=(15, 15))\n",
        "            plt.savefig(\"Approach_3Â°{}.png\".format(k-2))\n",
        "            plt.show()\n",
        "            \n",
        "    return(k_truss_nodes,k,G_bis,output_density)\n",
        "    \n",
        "k_truss_nodes,k,G_bis,output_density=k_truss_True_Dec_Massive_Net(G,output_for_density=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XWPKV3RKeq2Y",
        "outputId": "cb859681-cdc1-4dcb-efe8-90ad89ae10e6"
      },
      "outputs": [],
      "source": [
        "uw_some_word_list = closeness_centrality(G_unweighted)\n",
        "uw_some_words = []\n",
        "for key in uw_some_word_list.keys(): \n",
        "        uw_some_words.append(key)\n",
        "\n",
        "un_deg_list = degree_centrality(G_unweighted)\n",
        "un_deg_words = []\n",
        "for key in un_deg_list.keys(): \n",
        "        un_deg_words.append(key)\n",
        "\n",
        "\n",
        "sort_words = sorted(uw_some_word_list.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "import pandas as pd \n",
        "  \n",
        "# create DataFrame using data \n",
        "sw_df = pd.DataFrame(sort_words, columns =['Word','Score']) \n",
        "sw_df.to_csv('sort_words.csv')\n",
        "print(sw_df) \n",
        "\n",
        "for_plot = sw_df.iloc[0:100]\n",
        "for_plot.plot.density(color='Black')\n",
        "\n",
        "for_plot.plot.hist(color='Black')\n",
        "\n",
        "\n",
        "from networkx import pagerank\n",
        "pr_list = nx.pagerank(G_unweighted)\n",
        "pr_sort_words = sorted(pr_list.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "import pandas as pd \n",
        "  \n",
        "# create DataFrame using data \n",
        "pr_sw_df = pd.DataFrame(pr_sort_words, columns =['Word','Score']) \n",
        "pr_sw_df.to_csv('pr_sort_words.csv')\n",
        "print(pr_sw_df)  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
